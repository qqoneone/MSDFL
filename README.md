# Model Stability Defense against Model Poisoning in Federated Learning

This is the official implementation of our paper **Model Stability Defense against Model Poisoning in Federated Learning**.
This research project is developed based on Python 3 and Pytorch.

Authors: Qi Guo, Di Wu, Yong Qi, Saiyu Qi, Qian Li, Minghao Yao, and Kaitai Liang

Code contributor:  Di Wu, Qi Guo, and Minghao Yao

# Defense goals.

- Fidelity
- Robustness
- Compatibility
- Self-protection

# Produce experiments:

- There are various main files "mainAVG.py", "mainFedProx.py", "mainTrimean.py", "mainMedian.py", "mainBulyan.py", "mainFLTrust.py" which allow running different experiments.
- To run the experiments, access the main file and run: "python3 mainXXX.py"
